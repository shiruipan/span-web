---
title: "Adapting Membership Inference Attacks to GNN for Graph Classification: Approaches and Implications"
date: 2021-08-31
publishDate: 2021-08-31T12:15:49.436331Z
authors: ["Bang Wu", "Xiangwen Yang", "Shirui Pan", "Xingliang Yuan"]
publication_types: ["1"]
abstract: "In light of the wide application of Graph Neural Networks (GNNs), Membership Inference Attack (MIA) against GNNs raises severe privacy concerns, where training data can be leaked from trained GNN models. However, prior studies focus on inferring the membership of only the components in a graph, e.g., an individual node or edge. In this paper, we take the first step in MIA against GNNs for graph-level classification. Our objective is to infer whether a graph sample has been used for training a GNN model. We present and implement two types of attacks, i.e., training-based attacks and threshold-based attacks from different adversarial capabilities. We perform comprehensive experiments to evaluate our attacks in seven real-world datasets using five representative GNN models. Both our attacks are shown effective and can achieve high performance, i.e., reaching over 0:7 attack F1 scores in most cases1. Our findings also confirm that, unlike the node-level classifier, MIAs on graph-level classification tasks are more co-related with the overfitting level of GNNs rather than the statistic property of their training graphs."
featured: false
publication: "*IEEE International Conference on Data Mining (ICDM), Dec 7-10, 2021*"
tags: ["Graph Neural Networks", "Attacks"]
---
